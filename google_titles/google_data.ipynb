{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import langid\n",
    "\n",
    "\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import re\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis.sklearn\n",
    "#pyLDAvis.enable_notebook()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str):\n",
    "    with open(path, encoding = 'utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(dataset: pd.DataFrame) -> list:\n",
    "    titles = []\n",
    "    for i in dataset['Browser History']:\n",
    "            try:\n",
    "                 titles.append(i['title'])\n",
    "            except KeyError:\n",
    "                    pass\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt(dataset: list, name, path):\n",
    "\n",
    "    new_path = os.path.join(path, name)\n",
    "    f = open(new_path, 'a')\n",
    "    \n",
    "    for item in dataset:\n",
    "        without_line_breaks = item.replace(\"\\n\", \" \")\n",
    "        without_line_breaks = without_line_breaks.replace(\"\\r\", \" \")\n",
    "        lines = without_line_breaks + \"\\n\"\n",
    "        f.write(lines)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_letter(dataset: pd.DataFrame) -> list:\n",
    "    new_dataset = [re.sub('ø', 'oe', text) for text in dataset]\n",
    "    new_dataset = [re.sub('æ', 'ae', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('å', 'aa', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('Ø', 'oe', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('Æ', 'ae', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('Å', 'aa', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('ü', 'ue', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('Ü', 'ue', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('ä', 'ae', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('Ä', 'ae', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('ö', 'oe', text) for text in new_dataset]\n",
    "    new_dataset = [re.sub('Ö', 'oe', text) for text in new_dataset]\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(dataset: list, stops_da, stops_en) -> list:\n",
    "    no_urls = [re.sub(r\"http\\S+\", \"\", text) for text in dataset] \n",
    "    only_letters = [re.sub(r\"(#[A-Za-z]+)|(@[A-Za-z]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", ' ' , text) for text in no_urls] \n",
    "    only_letters = [text.replace('\\n', ' ') for text in only_letters]\n",
    "    lowercased_str = [text.lower().split() for text in only_letters] \n",
    "    no_stopwords = [[w for w in text if not w in stops_da] for text in lowercased_str] \n",
    "    no_stopwords = [[w for w in text if not w in stops_en] for text in no_stopwords] \n",
    "    cleaned_text = [\" \".join(text) for text in no_stopwords] \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(data):\n",
    "    tokens = [word_tokenize(text) for text in data] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(sent):\n",
    "    lemmas = [x.lemma_ for x in nlp(sent)]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_posts(tokenized_data: list):\n",
    "    lemmas = []\n",
    "    for post in tokenized_data:\n",
    "        lemma = [lemmatize_text(x) for x in post]\n",
    "        lemmas.append([item for sublist in lemma for item in sublist])\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_da(dataset: list) -> list:\n",
    "\n",
    "    langid.set_languages(['da', 'en'])  \n",
    "    da = []\n",
    "\n",
    "    for i in dataset:\n",
    "        lang = langid.classify(i)\n",
    "        if lang[0] != 'en':          \n",
    "            da.append(i)\n",
    "\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('data/google_data/BrowserHistoryAS.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13921\n"
     ]
    }
   ],
   "source": [
    "titles = extract_title(data)\n",
    "print(len(titles))\n",
    "\n",
    "#print(titles[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_txt(titles, name = 'all_titles.txt', path = 'data/google_data/prepared_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10388\n"
     ]
    }
   ],
   "source": [
    "da = only_da(titles)\n",
    "print(len(da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Psykolog dømt for seksuelt misbrug af incestof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indland | Nyheder Indland | Vi holder dig orie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seneste nyt – Få de seneste nyheder her | DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dr.dk nyheder - Google-søgning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dr.dk nyheder - Google-søgning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           row_posts\n",
       "0  Psykolog dømt for seksuelt misbrug af incestof...\n",
       "1  Indland | Nyheder Indland | Vi holder dig orie...\n",
       "2       Seneste nyt – Få de seneste nyheder her | DR\n",
       "3                     dr.dk nyheder - Google-søgning\n",
       "4                     dr.dk nyheder - Google-søgning"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(da, columns=['row_posts'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"stop_words.txt\",\"r+\")\n",
    "stop_words = file.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "stopwords_da = stopwords.words(\"danish\")  \n",
    "stopwords_da.extend(stop_words)\n",
    "\n",
    "nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "stopwords_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_new = change_letter(df['row_posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_titles = get_clean_text(titles_new, stops_da = stopwords_da, stops_en = stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_empty_strings = [string for string in cleaned_titles if string != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_titles= tokenize_text(remove_empty_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatized_titles = lemmatize_posts(tokenized_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(remove_empty_strings , columns=['cleaned_titles'])\n",
    "\n",
    "df1['tokenized_titles'] = tokenized_titles\n",
    "df1['lemmatized_titles'] = lemmatized_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_pickle('data/google_data/prepared_txt/all_titles_da_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df1['lemmatized_titles'])\n",
    "posts_bigrams = [bigram[line] for line in df1['lemmatized_titles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(posts_bigrams)\n",
    "corpus = [dictionary.doc2bow(text) for text in posts_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus = corpus, \n",
    "                    num_topics = 5, \n",
    "                    id2word = dictionary,\n",
    "                    update_every = 1,\n",
    "                    passes = 10,\n",
    "                    per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/home/kate/.local/share/virtualenvs/myproject-o7xzvy_i/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "model =pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\n",
    "#pyLDAvis.display(model)\n",
    "pyLDAvis.save_html(model, 'data/google_data/models/all_titles_da.html')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6805e15886aab53026236d0c7a2691c0f3a4936a9fa243a37c54dfd5efd3ad7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('myproject-o7xzvy_i': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
